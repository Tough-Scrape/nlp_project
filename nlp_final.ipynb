{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from requests import get\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import prep\n",
    "from env import github_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire \n",
    "\n",
    "Acuired most starred repos using BeautifulSoup to scrape repo links from the first 22 pages. Then used the Github API to pull the README.md files for each repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs list of repo names from most starred repo pages.\n",
    "def get_urls(start_page,stop_page):\n",
    "    pages = list(range(start_page,stop_page+1))\n",
    "    page_urls = []\n",
    "    for page in pages:\n",
    "        p = f'https://github.com/search?o=desc&p={page}&q=stars%3A%3E1&s=stars&type=Repositories'\n",
    "        page_urls.append(p)\n",
    "\n",
    "    repo_links_all = []\n",
    "    time.sleep(3)\n",
    "    for page in page_urls:\n",
    "        url = page\n",
    "        headers  = {'User-Agent' : 'Codeup Data Science Student'} #user-agent to decrease chance of being rejected by github\n",
    "        response = get(url,headers)\n",
    "        soup = BeautifulSoup(response.text) #created a BS object\n",
    "        repo_list = soup.find('ul', class_='repo-list') #find section of text of interest\n",
    "        links = repo_list.find_all(href = True) #isolate the 'href's \n",
    "        repo_links= [link['href'] for link in links if not re.match(r'^/topics',link['href']) and not re.match(r'^https',link['href'])\\\n",
    "                 and link['href'].count('/') < 3] #further pare down list by excluding anything that begins with /topics, https or more than 2 forward slashes\n",
    "        repo_links_all.extend(repo_links)\n",
    "    repo_links_all = [url[1:] for url in repo_links_all]\n",
    "    time.sleep(2)\n",
    "    return repo_links_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had problems scraping the website with the rest of the class also pinging the Github servers so we decided to save our urls and READMEs as files to be accessed locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('urls', 'rb') as fp:\n",
    "    urls = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>![freeCodeCamp.org Social Banner](https://s3.a...</td>\n",
       "      <td>freeCodeCamp/freeCodeCamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rust</td>\n",
       "      <td>[996.ICU](https://996.icu/#/en_US)\\n=======\\n*...</td>\n",
       "      <td>996icu/996.ICU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                    readme_contents  \\\n",
       "0  JavaScript  ![freeCodeCamp.org Social Banner](https://s3.a...   \n",
       "1        Rust  [996.ICU](https://996.icu/#/en_US)\\n=======\\n*...   \n",
       "\n",
       "                        repo  \n",
       "0  freeCodeCamp/freeCodeCamp  \n",
       "1             996icu/996.ICU  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data.json')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration\n",
    "\n",
    "We pulled 220 initial README files and grouped by the language. 32 had no language type and were dropped. After further discussion we decided to only work on classifying the top 5 most common languages of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "JavaScript    72\n",
      "Python        25\n",
      "Java          17\n",
      "C++           12\n",
      "Go            12\n",
      "TypeScript     9\n",
      "CSS            5\n",
      "C              4\n",
      "Name: repo, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(df.groupby('language')['repo'].count().sort_values(ascending=False).head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
